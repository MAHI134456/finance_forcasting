{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJtz83f2oNPz",
        "outputId": "e3d7c9c6-cebf-4b82-8031-d8b4f9afefba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwoNnwC3oZAq",
        "outputId": "27615b3b-036a-4d24-fa3c-7d65c5d88236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ Using file: /content/drive/MyDrive/data/cleaned/BND.csv\n",
            "Train: 2015-07-01 â†’ 2023-12-29 | Test: 2024-01-02 â†’ 2025-07-30 (395 days)\n",
            "ARIMA Predictions type: <class 'numpy.ndarray'>, shape: (395,)\n",
            "ARIMA Model type: <class 'statsmodels.tsa.arima.model.ARIMAResultsWrapper'>\n",
            "Epoch 1/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - loss: 0.0646 - val_loss: 0.0016\n",
            "Epoch 2/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 3/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 4/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 5/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 6/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 7/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 8/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 9/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 10/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 9.5369e-04 - val_loss: 0.0012\n",
            "Epoch 11/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - loss: 8.7743e-04 - val_loss: 0.0012\n",
            "Epoch 12/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - loss: 7.9023e-04 - val_loss: 0.0012\n",
            "Epoch 13/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 8.3322e-04 - val_loss: 0.0011\n",
            "Epoch 14/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 8.3116e-04 - val_loss: 0.0011\n",
            "Epoch 15/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - loss: 8.4524e-04 - val_loss: 0.0010\n",
            "Epoch 16/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - loss: 8.0764e-04 - val_loss: 0.0011\n",
            "Epoch 17/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 7.7736e-04 - val_loss: 0.0011\n",
            "Epoch 18/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 8.7207e-04 - val_loss: 0.0012\n",
            "Epoch 19/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 7.9722e-04 - val_loss: 0.0010\n",
            "Epoch 20/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 7.1210e-04 - val_loss: 9.5716e-04\n",
            "Epoch 21/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 8.2264e-04 - val_loss: 9.8241e-04\n",
            "Epoch 22/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 7.4835e-04 - val_loss: 9.1517e-04\n",
            "Epoch 23/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 6.8609e-04 - val_loss: 9.6838e-04\n",
            "Epoch 24/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - loss: 7.6240e-04 - val_loss: 9.9441e-04\n",
            "Epoch 25/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 74ms/step - loss: 7.1985e-04 - val_loss: 8.7729e-04\n",
            "Epoch 26/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - loss: 6.1470e-04 - val_loss: 8.6370e-04\n",
            "Epoch 27/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 6.4167e-04 - val_loss: 8.6993e-04\n",
            "Epoch 28/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 6.4037e-04 - val_loss: 8.2506e-04\n",
            "Epoch 29/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - loss: 6.4551e-04 - val_loss: 8.8646e-04\n",
            "Epoch 30/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 6.5640e-04 - val_loss: 8.4954e-04\n",
            "Epoch 31/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 6.2131e-04 - val_loss: 8.0064e-04\n",
            "Epoch 32/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - loss: 5.9376e-04 - val_loss: 7.6918e-04\n",
            "Epoch 33/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 6.1349e-04 - val_loss: 7.9326e-04\n",
            "Epoch 34/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 5.8897e-04 - val_loss: 7.8824e-04\n",
            "Epoch 35/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 5.8514e-04 - val_loss: 7.4223e-04\n",
            "Epoch 36/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - loss: 5.5559e-04 - val_loss: 7.9096e-04\n",
            "Epoch 37/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - loss: 6.3106e-04 - val_loss: 7.6884e-04\n",
            "Epoch 38/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 5.8365e-04 - val_loss: 7.2104e-04\n",
            "Epoch 39/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - loss: 5.8997e-04 - val_loss: 7.3173e-04\n",
            "Epoch 40/40\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 5.4215e-04 - val_loss: 6.8428e-04\n",
            "ğŸ’¾ Saved predictions: /content/drive/MyDrive/BND_forecast_outputs/predictions/BND_predictions.csv\n",
            "ğŸ“ˆ Saved: /content/drive/MyDrive/BND_forecast_outputs/plots/BND_actual_vs_forecasts.png\n",
            "ğŸ§ª Saved: /content/drive/MyDrive/BND_forecast_outputs/plots/BND_lstm_training_loss.png\n",
            "ğŸ“ Saved metrics report: /content/drive/MyDrive/BND_forecast_outputs/reports/BND_forecast_metrics.md\n"
          ]
        }
      ],
      "source": [
        "# scripts/forecast_tsla.py\n",
        "# Predict Tesla prices with ARIMA and LSTM, evaluate on a chronological split.\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle # Added for saving ARIMA model\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import statsmodels for ARIMA\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima.model import ARIMA as st_ARIMA # Renaming to avoid conflict\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "SPLIT_DATE = \"2024-01-01\"          # Train: < 2024-01-01, Test: >= 2024-01-01\n",
        "LOOKBACK = 60                      # LSTM sequence length (days)\n",
        "EPOCHS = 40                        # tweak as desired\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-3                          # LSTM learning rate\n",
        "SEED = 42\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "# SCRIPT_DIR = Path(__file__).resolve().parent # __file__ is not defined in Colab notebook\n",
        "# ROOT_DIR = SCRIPT_DIR.parent\n",
        "# CLEANED_DIR = ROOT_DIR / \"data\" / \"cleaned\"\n",
        "\n",
        "# Define paths relative to the Colab environment, assuming data is in Google Drive\n",
        "DRIVE_DIR = Path('/content/drive/MyDrive')\n",
        "CLEANED_DIR = DRIVE_DIR / \"data\" / \"cleaned\"\n",
        "OUT_DIR = DRIVE_DIR / \"BND_forecast_outputs\" # Use a dedicated output directory in Drive\n",
        "OUT_FORECASTS = OUT_DIR / \"forecasts\"\n",
        "OUT_PLOTS = OUT_DIR / \"plots\"\n",
        "OUT_REPORTS = OUT_DIR / \"reports\"\n",
        "OUT_PREDICTIONS = OUT_DIR / \"predictions\"\n",
        "OUT_MODELS = OUT_DIR / \"models\" # Added for saving models\n",
        "\n",
        "for p in [OUT_FORECASTS, OUT_PLOTS, OUT_REPORTS, OUT_PREDICTIONS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Utils\n",
        "# -----------------------------\n",
        "def set_seeds(seed=SEED):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(seed)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def find_tsla_csv(cleaned_dir: Path) -> Path:\n",
        "    # Prefer filenames containing 'TSLA'\n",
        "    candidates = [p for p in cleaned_dir.glob(\"*.csv\")]\n",
        "    bnd_like = [p for p in candidates if \"bnd\" in p.stem.lower()]\n",
        "    if bnd_like:\n",
        "        return bnd_like[0]\n",
        "    # Fallback: try to look into files and check ticker column if present\n",
        "    for p in candidates:\n",
        "        try:\n",
        "            head = pd.read_csv(p, nrows=5)\n",
        "            if any(c.lower() == \"ticker\" for c in head.columns):\n",
        "                if \"BND\" in head[\"Ticker\"].astype(str).unique():\n",
        "                    return p\n",
        "        except Exception:\n",
        "            continue\n",
        "    # Final fallback: first CSV (warn)\n",
        "    if candidates:\n",
        "        print(f\"âš ï¸ BND file not found by name. Using: {candidates[0].name}\")\n",
        "        return candidates[0]\n",
        "    raise FileNotFoundError(f\"No CSV files found in {cleaned_dir}\")\n",
        "\n",
        "def load_tsla_series(csv_path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Parse dates robustly\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "    price_col = \"Adj Close\" if \"Adj Close\" in df.columns else \"Close\"\n",
        "    if price_col not in df.columns:\n",
        "        raise ValueError(f\"{csv_path.name}: neither 'Adj Close' nor 'Close' present.\")\n",
        "    df[price_col] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[price_col]).copy()\n",
        "    df.rename(columns={price_col: \"Price\"}, inplace=True)\n",
        "    return df[[\"Date\", \"Price\"]]\n",
        "\n",
        "def train_test_split_chrono(df: pd.DataFrame, split_date: str):\n",
        "    train = df[df[\"Date\"] < split_date].copy()\n",
        "    test = df[df[\"Date\"] >= split_date].copy()\n",
        "    if len(train) == 0 or len(test) == 0:\n",
        "        raise ValueError(f\"Split resulted in empty sets. Check SPLIT_DATE={split_date}.\")\n",
        "    return train, test\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    rmse = math.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
        "    # R^2\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else np.nan\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape, \"sMAPE%\": smape, \"R2\": r2}\n",
        "\n",
        "\n",
        "def save_models(arima_model, lstm_model):\n",
        "    \"\"\"Saves the trained models to disk.\"\"\"\n",
        "    # Save ARIMA model\n",
        "    arima_path = OUT_MODELS / \"bnd_arima.pkl\"\n",
        "    with open(arima_path, \"wb\") as f:\n",
        "        pickle.dump(arima_model, f)\n",
        "    print(f\"ğŸ“¦ Saved ARIMA model: {arima_path}\")\n",
        "\n",
        "    # Save LSTM model\n",
        "    try:\n",
        "        lstm_path = OUT_MODELS / \"bnd_lstm.h5\"\n",
        "        lstm_model.save(lstm_path)\n",
        "        print(f\"ğŸ“¦ Saved LSTM model: {lstm_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not save LSTM model: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# ARIMA (using statsmodels)\n",
        "# -----------------------------\n",
        "def arima_forecast(train: pd.Series, horizon: int):\n",
        "    # Using a simple (5,1,0) ARIMA model as a starting point.\n",
        "    # More sophisticated order selection like auto_arima would require more code\n",
        "    # or a different library, but this avoids the pmdarima conflict.\n",
        "    order = (5, 1, 0)\n",
        "    model = st_ARIMA(train, order=order)\n",
        "    model_fit = model.fit()\n",
        "    # Forecast includes the training data points that are used for prediction.\n",
        "    # We only need the forecast for the horizon.\n",
        "    preds = model_fit.forecast(steps=horizon)\n",
        "    return preds.values, model_fit # Return values as numpy array\n",
        "\n",
        "# -----------------------------\n",
        "# LSTM\n",
        "# -----------------------------\n",
        "def make_sequences(arr, lookback):\n",
        "    X, y = [], []\n",
        "    for i in range(lookback, len(arr)):\n",
        "        X.append(arr[i - lookback:i, 0])\n",
        "        y.append(arr[i, 0])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X[..., np.newaxis], y  # shape: (samples, lookback, 1), (samples,)\n",
        "\n",
        "def lstm_forecast(train_prices: pd.Series, test_prices: pd.Series, lookback=LOOKBACK):\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.keras import Sequential\n",
        "        from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "        from tensorflow.keras.callbacks import EarlyStopping\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "    except ImportError:\n",
        "        raise SystemExit(\"Please install TensorFlow and scikit-learn: pip install tensorflow scikit-learn\")\n",
        "\n",
        "    # Scale (fit on train only)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    train_scaled = scaler.fit_transform(train_prices.values.reshape(-1, 1))\n",
        "    test_scaled = scaler.transform(test_prices.values.reshape(-1, 1))\n",
        "\n",
        "    # Build sequences\n",
        "    X_train, y_train = make_sequences(train_scaled, lookback)\n",
        "\n",
        "    # For test, prepend the last lookback points from training to construct windows\n",
        "    combo = np.vstack([train_scaled[-lookback:], test_scaled])\n",
        "    X_test, y_test_scaled = make_sequences(combo, lookback)\n",
        "\n",
        "    # Model\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(SEED)\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(lookback, 1)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "    model.compile(optimizer=opt, loss=\"mse\")\n",
        "\n",
        "    es = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "    hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.1,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[es],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Predict (scaled), then invert scaling\n",
        "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled).ravel()\n",
        "\n",
        "    # Align to test dates (one prediction per test day)\n",
        "    return y_pred, model, hist\n",
        "\n",
        "# -----------------------------\n",
        "# Plotting\n",
        "# -----------------------------\n",
        "def plot_forecasts(df, train, test, arima_pred, lstm_pred):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df[\"Date\"], df[\"Price\"], label=\"Actual\", linewidth=1.2)\n",
        "    plt.axvline(pd.Timestamp(SPLIT_DATE), linestyle=\"--\", label=\"Split\", alpha=0.7)\n",
        "    # Overlay predictions on test period\n",
        "    test_dates = test[\"Date\"].values\n",
        "    plt.plot(test_dates, arima_pred, label=\"ARIMA Forecast\")\n",
        "    plt.plot(test_dates, lstm_pred, label=\"LSTM Forecast\")\n",
        "    plt.title(\"BND â€” Actual vs Forecasts (ARIMA & LSTM)\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Price\")\n",
        "    plt.legend(); plt.grid(True)\n",
        "    out_path = OUT_PLOTS / \"BND_actual_vs_forecasts.png\"\n",
        "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
        "    print(f\"ğŸ“ˆ Saved: {out_path}\")\n",
        "\n",
        "def plot_lstm_training(history):\n",
        "    try:\n",
        "        loss = history.history[\"loss\"]\n",
        "        val_loss = history.history.get(\"val_loss\")\n",
        "    except Exception:\n",
        "        return\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(loss, label=\"Train Loss\")\n",
        "    if val_loss is not None:\n",
        "        plt.plot(val_loss, label=\"Val Loss\")\n",
        "    plt.title(\"LSTM Training Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\")\n",
        "    plt.legend(); plt.grid(True)\n",
        "    out_path = OUT_PLOTS / \"BND_lstm_training_loss.png\"\n",
        "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
        "    print(f\"ğŸ§ª Saved: {out_path}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "def main():\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    set_seeds(SEED)\n",
        "\n",
        "    tsla_csv = find_tsla_csv(CLEANED_DIR)\n",
        "    print(f\"ğŸ“‚ Using file: {tsla_csv}\")\n",
        "\n",
        "    df = load_tsla_series(tsla_csv)\n",
        "    train, test = train_test_split_chrono(df, SPLIT_DATE)\n",
        "    print(f\"Train: {train['Date'].iloc[0].date()} â†’ {train['Date'].iloc[-1].date()} | \"\n",
        "          f\"Test: {test['Date'].iloc[0].date()} â†’ {test['Date'].iloc[-1].date()} \"\n",
        "          f\"({len(test)} days)\")\n",
        "\n",
        "    # ---------- ARIMA ----------\n",
        "    arima_pred, arima_model = arima_forecast(train[\"Price\"], horizon=len(test))\n",
        "    print(f\"ARIMA Predictions type: {type(arima_pred)}, shape: {arima_pred.shape if hasattr(arima_pred, 'shape') else 'N/A'}\")\n",
        "    print(f\"ARIMA Model type: {type(arima_model)}\")\n",
        "    arima_metrics = metrics(test[\"Price\"].values, arima_pred)\n",
        "\n",
        "    # ---------- LSTM ----------\n",
        "    lstm_pred, lstm_model, hist = lstm_forecast(train[\"Price\"], test[\"Price\"], lookback=LOOKBACK)\n",
        "    lstm_metrics = metrics(test[\"Price\"].values, lstm_pred)\n",
        "\n",
        "    # ---------- Save models ----------\n",
        "    save_models(arima_model, lstm_model)\n",
        "\n",
        "    # ---------- Save predictions ----------\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"Date\": test[\"Date\"].values,\n",
        "        \"Actual\": test[\"Price\"].values,\n",
        "        \"ARIMA_Pred\": arima_pred,\n",
        "        \"LSTM_Pred\": lstm_pred\n",
        "    })\n",
        "    out_pred = OUT_PREDICTIONS / \"BND_predictions.csv\"\n",
        "    pred_df.to_csv(out_pred, index=False)\n",
        "    print(f\"ğŸ’¾ Saved predictions: {out_pred}\")\n",
        "\n",
        "    # ---------- Plots ----------\n",
        "    plot_forecasts(df, train, test, arima_pred, lstm_pred)\n",
        "    plot_lstm_training(hist)\n",
        "\n",
        "    # ---------- Metrics report ----------\n",
        "    md = f\"\"\"# BND Forecast â€” ARIMA vs LSTM\n",
        "\n",
        "**Train period:** {train['Date'].iloc[0].date()} â†’ {train['Date'].iloc[-1].date()}\n",
        "**Test period:** {test['Date'].iloc[0].date()} â†’ {test['Date'].iloc[-1].date()}\n",
        "**Horizon:** {len(test)} trading days\n",
        "\n",
        "## Test Metrics\n",
        "### ARIMA\n",
        "- MAE:   {arima_metrics['MAE']:.4f}\n",
        "- RMSE:  {arima_metrics['RMSE']:.4f}\n",
        "- MAPE:  {arima_metrics['MAPE%']:.2f}%\n",
        "- sMAPE: {arima_metrics['sMAPE%']:.2f}%\n",
        "- RÂ²:    {arima_metrics['R2']:.4f}\n",
        "\n",
        "### LSTM\n",
        "- MAE:   {lstm_metrics['MAE']:.4f}\n",
        "- RMSE:  {lstm_metrics['RMSE']:.4f}\n",
        "- MAPE:  {lstm_metrics['MAPE%']:.2f}%\n",
        "- sMAPE: {lstm_metrics['sMAPE%']:.2f}%\n",
        "- RÂ²:    {lstm_metrics['R2']:.4f}\n",
        "\n",
        "## Notes\n",
        "- Data split is **chronological**; no shuffling.\n",
        "- ARIMA provides interpretability (orders, residual diagnostics), while LSTM can capture **non-linear** patterns at the cost of more data & tuning.\n",
        "- Consider forecasting **returns** and integrating other features (volume, macro, sector ETF) if you want to push accuracy further.\n",
        "\"\"\"\n",
        "    report_path = OUT_REPORTS / \"BND_forecast_metrics.md\"\n",
        "    report_path.write_text(md, encoding=\"utf-8\")\n",
        "    print(f\"ğŸ“ Saved metrics report: {report_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Soft dependency checks with helpful messages\n",
        "    try:\n",
        "        import pandas as pd  # noqa\n",
        "        import statsmodels   # noqa\n",
        "    except ImportError:\n",
        "        print(\"Please install requirements: pip install pandas statsmodels tensorflow scikit-learn matplotlib\")\n",
        "        raise\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
